{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the dataset into train and test\n",
    "df_train = pd.read_csv('C://Users//taksaa//5/blood_train.csv')\n",
    "df_test = pd.read_csv('C://Users//taksaa//5/blood_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shuffling both the dataframes and resetting the index\n",
    "df_train= df_train.sample(frac=1).reset_index(drop=True)\n",
    "df_test=df_test.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Recency (months)  Frequency (times)  Monetary (c.c. blood)  Time (months)  \\\n",
      "0                 4                 17                   4250             71   \n",
      "1                23                  4                   1000             52   \n",
      "2                14                  7                   1750             72   \n",
      "3                23                  1                    250             23   \n",
      "4                14                  1                    250             14   \n",
      "\n",
      "   whether he/she donated blood in March 2007  \n",
      "0                                           1  \n",
      "1                                           0  \n",
      "2                                           0  \n",
      "3                                           0  \n",
      "4                                           0  \n",
      "   Recency (months)  Frequency (times)  Monetary (c.c. blood)  Time (months)  \\\n",
      "0                 2                  3                    750              4   \n",
      "1                 2                  1                    250              2   \n",
      "2                 2                  4                   1000             26   \n",
      "3                 4                 19                   4750             69   \n",
      "4                 2                  5                   1250             34   \n",
      "\n",
      "   whether he/she donated blood in March 2007  \n",
      "0                                           1  \n",
      "1                                           0  \n",
      "2                                           0  \n",
      "3                                           1  \n",
      "4                                           0  \n",
      "599\n",
      "149\n"
     ]
    }
   ],
   "source": [
    "#viewing top 5 rows of the dataframe using head and Size of the dataframes\n",
    "print(df_train.head());print(df_test.head());\n",
    "print(len(df_train));\n",
    "print(len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaling/standardizing  the datasets\n",
    "from sklearn import preprocessing\n",
    "df_train.iloc[:,0:4]=preprocessing.scale(df_train.iloc[:,0:4])\n",
    "df_test.iloc[:,0:4]=preprocessing.scale(df_test.iloc[:,0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for precision\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "C:\\Users\\taksaa\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.723 (+/-0.106) for {'C': 1, 'gamma': 10, 'kernel': 'rbf'}\n",
      "\n",
      "0.732 (+/-0.148) for {'C': 1, 'gamma': 1, 'kernel': 'rbf'}\n",
      "\n",
      "0.381 (+/-0.002) for {'C': 1, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "0.381 (+/-0.002) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "0.657 (+/-0.057) for {'C': 10, 'gamma': 10, 'kernel': 'rbf'}\n",
      "\n",
      "0.746 (+/-0.165) for {'C': 10, 'gamma': 1, 'kernel': 'rbf'}\n",
      "\n",
      "0.786 (+/-0.244) for {'C': 10, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "0.381 (+/-0.002) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "0.672 (+/-0.081) for {'C': 100, 'gamma': 10, 'kernel': 'rbf'}\n",
      "\n",
      "0.709 (+/-0.126) for {'C': 100, 'gamma': 1, 'kernel': 'rbf'}\n",
      "\n",
      "0.712 (+/-0.245) for {'C': 100, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "0.483 (+/-0.403) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "0.645 (+/-0.094) for {'C': 1000, 'gamma': 10, 'kernel': 'rbf'}\n",
      "\n",
      "0.662 (+/-0.054) for {'C': 1000, 'gamma': 1, 'kernel': 'rbf'}\n",
      "\n",
      "0.795 (+/-0.155) for {'C': 1000, 'gamma': 0.01, 'kernel': 'rbf'}\n",
      "\n",
      "0.740 (+/-0.367) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "0.583 (+/-0.377) for {'C': 1, 'kernel': 'linear'}\n",
      "\n",
      "0.583 (+/-0.377) for {'C': 10, 'kernel': 'linear'}\n",
      "\n",
      "0.583 (+/-0.377) for {'C': 100, 'kernel': 'linear'}\n",
      "\n",
      "0.583 (+/-0.377) for {'C': 1000, 'kernel': 'linear'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.82      0.97      0.89       457\n",
      "          1       0.75      0.32      0.45       142\n",
      "\n",
      "avg / total       0.80      0.81      0.78       599\n",
      "\n",
      "\n",
      "# Tuning hyper-parameters for recall\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#hyper tuning svm kernel and parameters with 5 fold CV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [10,1,1e-2,1e-3],'C': [1, 10, 100, 1000]},\n",
    "                    {'kernel': ['linear'], 'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=5,scoring='%s_macro' % score,n_jobs=1)\n",
    "\n",
    "    clf.fit(df_train.iloc[:,0:4],df_train.iloc[:,4])\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                % (mean, std * 2, params))\n",
    "        print()\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = df_train.iloc[:,4], clf.predict(df_train.iloc[:,0:4])\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypertuning parameters of svm with linear kernel\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'C': [1, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clfx = GridSearchCV(LinearSVC(penalty=\"l1\",dual=False), tuned_parameters, cv=5,scoring='%s_macro' % score,n_jobs=1)\n",
    "\n",
    "    clfx.fit(df_train.iloc[:,0:4],df_train.iloc[:,4])\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clfx.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clfx.cv_results_['mean_test_score']\n",
    "    stds = clfx.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clfx.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                % (mean, std * 2, params))\n",
    "        print()\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = df_train.iloc[:,4], clfx.predict(df_train.iloc[:,0:4])\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true, y_test_pred = df_test.iloc[:,4], clfx.predict(df_test.iloc[:,0:4])\n",
    "y_train_true, y_train_pred = df_train.iloc[:,4], clfx.predict(df_train.iloc[:,0:4])\n",
    "#confusion matrix for train and test set\n",
    "print('confusion matrix train \\n',metrics.confusion_matrix(y_train_true, y_train_pred))\n",
    "print('confusion matrix test \\n',metrics.confusion_matrix(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_final = SVC(kernel='rbf', C=10, gamma=1)\n",
    "clf_final.fit(df_train.iloc[:,0:4],df_train.iloc[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true, y_test_pred = df_test.iloc[:,4], clf_final.predict(df_test.iloc[:,0:4])\n",
    "y_train_true, y_train_pred = df_train.iloc[:,4], clf_final.predict(df_train.iloc[:,0:4])\n",
    "#confusion matrix for train and test set\n",
    "print('confusion matrix train \\n',metrics.confusion_matrix(y_train_true, y_train_pred))\n",
    "print('confusion matrix test \\n',metrics.confusion_matrix(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#auc\n",
    "print('auc train',metrics.roc_auc_score(y_train_true, y_train_pred))\n",
    "print('auc test',metrics.roc_auc_score(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#accuracy\n",
    "print('Accuracy train',metrics.accuracy_score(y_train_true, y_train_pred))\n",
    "print('Accuracy test',metrics.accuracy_score(y_test_true, y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating new dataset for semi_supervised learning. Our labelled data needs to contain 50% positive\n",
    "#50% negative labels.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "labelled_data_1, unlabelled_data_1= train_test_split(df_train[df_train.iloc[:,4]==1],test_size=0.50)\n",
    "labelled_data_0, unlabelled_data_0= train_test_split(df_train[df_train.iloc[:,4]==0],test_size=0.50)\n",
    "\n",
    "# combining and reshuffling the dataFrames\n",
    "labelled_data=labelled_data_0.append(labelled_data_1)\n",
    "unlabelled_data= unlabelled_data_0.append(unlabelled_data_1)\n",
    "#dropping labels from unlabelled dataFrame\n",
    "unlabelled_data=unlabelled_data.iloc[:,0:4]\n",
    "#shuffling the data frames\n",
    "labelled_data=pd.DataFrame(labelled_data.sample(frac=1).reset_index(drop=True))\n",
    "unlabelled_data=pd.DataFrame(unlabelled_data.sample(frac=1).reset_index(drop=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hypertuning parameters of svm with linear kernel\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'C': [1,3,6, 10, 100, 1000]}]\n",
    "\n",
    "scores = ['precision', 'recall']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf2 = GridSearchCV(LinearSVC(penalty=\"l1\",dual=False), tuned_parameters, cv=5,scoring='%s_macro' % score,n_jobs=1)\n",
    "\n",
    "    clf2.fit(labelled_data.iloc[:,0:4],labelled_data.iloc[:,4])\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf2.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf2.cv_results_['mean_test_score']\n",
    "    stds = clf2.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf2.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "                % (mean, std * 2, params))\n",
    "        print()\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = labelled_data.iloc[:,4], clf2.predict(labelled_data.iloc[:,0:4])\n",
    "    print(classification_report(y_true, y_pred))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the unlabeled data point that is the closest to the decision boundary\n",
    "#of the SVM.\n",
    "import numpy as np\n",
    "\n",
    "def closestN(X_array, n):\n",
    "    # array of sample distances to the hyperplane\n",
    "    dists = clf2.decision_function(X_array)\n",
    "    # absolute distance to hyperplane\n",
    "    absdists = np.abs(dists)\n",
    "\n",
    "    return pd.DataFrame(absdists.argsort()[:n])\n",
    "\n",
    "samples_wrt_planes = closestN(unlabelled_data, len(unlabelled_data))\n",
    "#getting the closest data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples_wrt_planes.iloc[90]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=clf2.predict(unlabelled_data.iloc[samples_wrt_planes.iloc[0],0:4])\n",
    "dx=pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx.append(unlabelled_data.iloc[samples_wrt_planes.iloc[0],0:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx.append(labelled_data.iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_data.append(unlabelled_data.iloc[samples_wrt_planes.iloc[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label=clf2.predict(unlabelled_data.iloc[samples_wrt_planes[0],0:4])\n",
    "labelled_data.append(unlabelled_data.iloc[samples_wrt_planes.iloc[0]])\n",
    "labelled_data.iloc[-1,4]=label\n",
    "\n",
    "clf2.fit(labelled_data.iloc[:,0:4],labelled_data.iloc[:,4])\n",
    "y_true, y_pred = labelled_data.iloc[:,4], clf2.predict(labelled_data.iloc[:,0:4])\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(samples_wrt_planes)):\n",
    "    label=int(clf2.predict(unlabelled_data.iloc[samples_wrt_planes[i],0:4]))\n",
    "    labelled_data.append(unlabelled_data.iloc[samples_wrt_planes.iloc[i],0:4])\n",
    "    labelled_data.loc[-1,4]=label\n",
    "    clf2.fit(labelled_data.iloc[:,0:4],labelled_data.iloc[:,4])\n",
    "    y_true, y_pred = labelled_data.iloc[:,4], clf2.predict(labelled_data.iloc[:,0:4])\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_true, y_test_pred = df_test.iloc[:,4], clf2.predict(df_test.iloc[:,0:4])\n",
    "print('confusion matrix test \\n',metrics.confusion_matrix(y_test_true, y_test_pred));\n",
    "print('auc test',metrics.roc_auc_score(y_test_true, y_test_pred));\n",
    "print('Accuracy test',metrics.accuracy_score(y_test_true, y_test_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
