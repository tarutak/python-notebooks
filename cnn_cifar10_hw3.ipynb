{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cnn.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "kVKbKxg6l-Vt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Home 3: Build a CNN for image recognition.\n",
        "\n",
        "### Name: Taru Tak\n"
      ]
    },
    {
      "metadata": {
        "id": "y0diqbu4l-Vz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 0. You will do the following:\n",
        "\n",
        "1. Read, complete, and run my code.\n",
        "\n",
        "2. **Make substantial improvements** to maximize the accurcy.\n",
        "    \n",
        "3. Convert the .IPYNB file to .HTML file.\n",
        "\n",
        "    * The HTML file must contain the code and the output after execution.\n",
        "    \n",
        "4. Upload this .HTML file to your Github repo.\n",
        "\n",
        "4. Submit the link to this .HTML file to Canvas.\n",
        "\n",
        "    * Example: https://github.com/wangshusen/CS583A-2019Spring/blob/master/homework/HM3/cnn.html\n"
      ]
    },
    {
      "metadata": {
        "id": "0jDUv9cEl-V4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "metadata": {
        "id": "w1Q9k6KVl-V8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.1. Load data\n"
      ]
    },
    {
      "metadata": {
        "id": "BoYqdEP2l-WS",
        "colab_type": "code",
        "outputId": "61919016-b8b6-4f91-c70c-ca53c398eb3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import cifar10\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('shape of x_train: ' + str(x_train.shape))\n",
        "print('shape of y_train: ' + str(y_train.shape))\n",
        "print('shape of x_test: ' + str(x_test.shape))\n",
        "print('shape of y_test: ' + str(y_test.shape))\n",
        "print('number of classes: ' + str(numpy.max(y_train) - numpy.min(y_train) + 1))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 22s 0us/step\n",
            "shape of x_train: (50000, 32, 32, 3)\n",
            "shape of y_train: (50000, 1)\n",
            "shape of x_test: (10000, 32, 32, 3)\n",
            "shape of y_test: (10000, 1)\n",
            "number of classes: 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BNA-eKs1l-Wn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.2. One-hot encode the labels\n",
        "\n",
        "In the input, a label is a scalar in $\\{0, 1, \\cdots , 9\\}$. One-hot encode transform such a scalar to a $10$-dim vector. E.g., a scalar ```y_train[j]=3``` is transformed to the vector ```y_train_vec[j]=[0, 0, 0, 1, 0, 0, 0, 0, 0, 0]```.\n",
        "\n",
        "1. Define a function ```to_one_hot``` that transforms an $n\\times 1$ array to a $n\\times 10$ matrix.\n",
        "\n",
        "2. Apply the function to ```y_train``` and ```y_test```."
      ]
    },
    {
      "metadata": {
        "id": "xyX6Al0El-Wr",
        "colab_type": "code",
        "outputId": "34382706-e4d7-417c-a917-354a79b6c513",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "def to_one_hot(y, num_class=10):\n",
        "    targets = numpy.array(y).reshape(-1)\n",
        "    return numpy.eye(num_class)[targets]\n",
        "\n",
        "y_train_vec = to_one_hot(y_train)\n",
        "y_test_vec = to_one_hot(y_test)\n",
        "\n",
        "print('Shape of y_train_vec: ' + str(y_train_vec.shape))\n",
        "print('Shape of y_test_vec: ' + str(y_test_vec.shape))\n",
        "\n",
        "print(y_train[0])\n",
        "print(y_train_vec[0])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of y_train_vec: (50000, 10)\n",
            "Shape of y_test_vec: (10000, 10)\n",
            "[6]\n",
            "[0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nylxiwD6l-W6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### Remark: the outputs should be\n",
        "* Shape of y_train_vec: (50000, 10)\n",
        "* Shape of y_test_vec: (10000, 10)\n",
        "* [6]\n",
        "* [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]"
      ]
    },
    {
      "metadata": {
        "id": "X--g8MDXl-W-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3. Randomly partition the training set to training and validation sets\n",
        "\n",
        "Randomly partition the 50K training samples to 2 sets:\n",
        "* a training set containing 40K samples\n",
        "* a validation set containing 10K samples\n"
      ]
    },
    {
      "metadata": {
        "id": "D8AsVCL0l-XB",
        "colab_type": "code",
        "outputId": "bde7e766-e71b-4897-d4d9-adacf87f193e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "cell_type": "code",
      "source": [
        "rand_indices = numpy.random.permutation(50000)\n",
        "train_indices = rand_indices[0:40000]\n",
        "valid_indices = rand_indices[40000:50000]\n",
        "\n",
        "x_val = x_train[valid_indices, :]\n",
        "y_val = y_train_vec[valid_indices, :]\n",
        "\n",
        "x_tr = x_train[train_indices, :]\n",
        "y_tr = y_train_vec[train_indices, :]\n",
        "\n",
        "print('Shape of x_tr: ' + str(x_tr.shape))\n",
        "print('Shape of y_tr: ' + str(y_tr.shape))\n",
        "print('Shape of x_val: ' + str(x_val.shape))\n",
        "print('Shape of y_val: ' + str(y_val.shape))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of x_tr: (40000, 32, 32, 3)\n",
            "Shape of y_tr: (40000, 10)\n",
            "Shape of x_val: (10000, 32, 32, 3)\n",
            "Shape of y_val: (10000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dTwEcHO7l-XQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Build a CNN and tune its hyper-parameters\n",
        "\n",
        "1. Build a convolutional neural network model\n",
        "2. Use the validation data to tune the hyper-parameters (e.g., network structure, and optimization algorithm)\n",
        "    * Do NOT use test data for hyper-parameter tuning!!!\n",
        "3. Try to achieve a validation accuracy as high as possible."
      ]
    },
    {
      "metadata": {
        "id": "OLRHydZgl-XV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Remark: \n",
        "\n",
        "The following CNN is just an example. You are supposed to make **substantial improvements** such as:\n",
        "* Add more layers.\n",
        "* Use regularizations, e.g., dropout.\n",
        "* Use batch normalization."
      ]
    },
    {
      "metadata": {
        "id": "HeVMBrYs_aXA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras import layers\n",
        "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation,BatchNormalization,Dropout\n",
        "from keras.models import Sequential\n",
        "from keras import regularizers, optimizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Lp0-cLVaKMbr",
        "colab_type": "code",
        "outputId": "34deefc7-5cf5-43e0-d079-b8967205cdea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1192
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3,3), padding='same', input_shape=x_train.shape[1:]))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(32, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.2))\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(64, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.3))\n",
        "\n",
        "model.add(Conv2D(128, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Conv2D(128, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Dropout(0.4))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 32, 32, 32)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 8, 8, 128)         0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                20490     \n",
            "=================================================================\n",
            "Total params: 309,290\n",
            "Trainable params: 308,394\n",
            "Non-trainable params: 896\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3zg6ceEgJTAl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=False\n",
        "    )\n",
        "datagen.fit(x_tr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pDgqTVZIC_ns",
        "colab_type": "code",
        "outputId": "178b320d-7e72-4642-afe4-061e15593b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2721
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "epochs=75\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer=optimizers.rmsprop(lr=0.001),\n",
        "        metrics=['accuracy'])\n",
        "history=model.fit_generator(datagen.flow(x_tr, y_tr, batch_size=batch_size),steps_per_epoch = len(x_tr) / 128,epochs=epochs,verbose=1,validation_data=(x_val,y_val))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Epoch 1/75\n",
            "313/312 [==============================] - 32s 103ms/step - loss: 1.9018 - acc: 0.3802 - val_loss: 1.8788 - val_acc: 0.3891\n",
            "Epoch 2/75\n",
            "313/312 [==============================] - 26s 83ms/step - loss: 1.4848 - acc: 0.5082 - val_loss: 1.4944 - val_acc: 0.5248\n",
            "Epoch 3/75\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 1.3072 - acc: 0.5687 - val_loss: 1.3553 - val_acc: 0.5520\n",
            "Epoch 4/75\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 1.1755 - acc: 0.6152 - val_loss: 1.2658 - val_acc: 0.5655\n",
            "Epoch 5/75\n",
            "313/312 [==============================] - 26s 82ms/step - loss: 1.1062 - acc: 0.6433 - val_loss: 1.0137 - val_acc: 0.6625\n",
            "Epoch 6/75\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 1.0220 - acc: 0.6671 - val_loss: 0.9508 - val_acc: 0.6967\n",
            "Epoch 7/75\n",
            "313/312 [==============================] - 26s 83ms/step - loss: 0.9856 - acc: 0.6824 - val_loss: 0.9642 - val_acc: 0.6905\n",
            "Epoch 8/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.9740 - acc: 0.6921 - val_loss: 0.8414 - val_acc: 0.7214\n",
            "Epoch 9/75\n",
            "313/312 [==============================] - 26s 83ms/step - loss: 0.9058 - acc: 0.7103 - val_loss: 0.9282 - val_acc: 0.6890\n",
            "Epoch 10/75\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.9426 - acc: 0.7116 - val_loss: 1.1584 - val_acc: 0.6450\n",
            "Epoch 11/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.8742 - acc: 0.7231 - val_loss: 1.2598 - val_acc: 0.6235\n",
            "Epoch 12/75\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.8564 - acc: 0.7300 - val_loss: 0.8047 - val_acc: 0.7443\n",
            "Epoch 13/75\n",
            "313/312 [==============================] - 26s 82ms/step - loss: 0.8443 - acc: 0.7351 - val_loss: 0.8762 - val_acc: 0.7198\n",
            "Epoch 14/75\n",
            "313/312 [==============================] - 28s 89ms/step - loss: 0.8155 - acc: 0.7417 - val_loss: 0.9468 - val_acc: 0.7244\n",
            "Epoch 15/75\n",
            "313/312 [==============================] - 26s 83ms/step - loss: 0.7980 - acc: 0.7452 - val_loss: 0.8385 - val_acc: 0.7183\n",
            "Epoch 16/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.7829 - acc: 0.7481 - val_loss: 0.8595 - val_acc: 0.7323\n",
            "Epoch 17/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.7699 - acc: 0.7584 - val_loss: 0.8026 - val_acc: 0.7506\n",
            "Epoch 18/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.7635 - acc: 0.7580 - val_loss: 0.7248 - val_acc: 0.7649\n",
            "Epoch 19/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.7521 - acc: 0.7651 - val_loss: 0.6745 - val_acc: 0.7865\n",
            "Epoch 20/75\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.7546 - acc: 0.7655 - val_loss: 0.6565 - val_acc: 0.7798\n",
            "Epoch 21/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.7437 - acc: 0.7691 - val_loss: 0.7431 - val_acc: 0.7668\n",
            "Epoch 22/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.7196 - acc: 0.7749 - val_loss: 0.7972 - val_acc: 0.7657\n",
            "Epoch 23/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.7171 - acc: 0.7770 - val_loss: 0.6808 - val_acc: 0.7845\n",
            "Epoch 24/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.6932 - acc: 0.7822 - val_loss: 0.6167 - val_acc: 0.8000\n",
            "Epoch 26/75\n",
            "313/312 [==============================] - 28s 90ms/step - loss: 0.6841 - acc: 0.7845 - val_loss: 0.6075 - val_acc: 0.8034\n",
            "Epoch 27/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.6827 - acc: 0.7868 - val_loss: 0.6516 - val_acc: 0.7945\n",
            "Epoch 28/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.6817 - acc: 0.7868 - val_loss: 0.7158 - val_acc: 0.7787\n",
            "Epoch 29/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.6636 - acc: 0.7935 - val_loss: 0.8493 - val_acc: 0.7289\n",
            "Epoch 30/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.6992 - acc: 0.7868 - val_loss: 0.5428 - val_acc: 0.8216\n",
            "Epoch 31/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.7029 - acc: 0.7893 - val_loss: 0.8698 - val_acc: 0.7394\n",
            "Epoch 32/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.6674 - acc: 0.7920 - val_loss: 0.5406 - val_acc: 0.8241\n",
            "Epoch 33/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.6495 - acc: 0.7960 - val_loss: 0.7414 - val_acc: 0.7643\n",
            "Epoch 34/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.6620 - acc: 0.7955 - val_loss: 0.7414 - val_acc: 0.7693\n",
            "Epoch 35/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.6646 - acc: 0.7964 - val_loss: 0.7420 - val_acc: 0.7673\n",
            "Epoch 36/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.6570 - acc: 0.7993 - val_loss: 0.6347 - val_acc: 0.7976\n",
            "Epoch 37/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.6637 - acc: 0.7971 - val_loss: 0.6720 - val_acc: 0.7783\n",
            "Epoch 38/75\n",
            "313/312 [==============================] - 28s 89ms/step - loss: 0.6385 - acc: 0.8045 - val_loss: 0.7343 - val_acc: 0.7836\n",
            "Epoch 39/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.6300 - acc: 0.8041 - val_loss: 0.5827 - val_acc: 0.8165\n",
            "Epoch 40/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.6329 - acc: 0.8050 - val_loss: 0.6265 - val_acc: 0.7992\n",
            "Epoch 41/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.6280 - acc: 0.8071 - val_loss: 0.5693 - val_acc: 0.8242\n",
            "Epoch 42/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.6574 - acc: 0.8018 - val_loss: 0.5543 - val_acc: 0.8210\n",
            "Epoch 43/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.7740 - acc: 0.7890 - val_loss: 0.6613 - val_acc: 0.7972\n",
            "Epoch 44/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.7493 - acc: 0.7910 - val_loss: 0.5413 - val_acc: 0.8234\n",
            "Epoch 45/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.6460 - acc: 0.8044 - val_loss: 0.6735 - val_acc: 0.7946\n",
            "Epoch 46/75\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.6320 - acc: 0.8065 - val_loss: 0.6044 - val_acc: 0.8131\n",
            "Epoch 47/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.6099 - acc: 0.8087 - val_loss: 0.4735 - val_acc: 0.8501\n",
            "Epoch 48/75\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.6033 - acc: 0.8120 - val_loss: 0.5400 - val_acc: 0.8284\n",
            "Epoch 49/75\n",
            "313/312 [==============================] - 26s 82ms/step - loss: 0.6234 - acc: 0.8122 - val_loss: 0.5797 - val_acc: 0.8114\n",
            "Epoch 50/75\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 0.6070 - acc: 0.8165 - val_loss: 0.6285 - val_acc: 0.8102\n",
            "Epoch 51/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.5895 - acc: 0.8167 - val_loss: 0.6284 - val_acc: 0.8094\n",
            "Epoch 52/75\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.5974 - acc: 0.8184 - val_loss: 0.5084 - val_acc: 0.8455\n",
            "Epoch 53/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.6135 - acc: 0.8145 - val_loss: 0.4635 - val_acc: 0.8459\n",
            "Epoch 54/75\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.6019 - acc: 0.8185 - val_loss: 0.4927 - val_acc: 0.8377\n",
            "Epoch 55/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.6089 - acc: 0.8163 - val_loss: 0.8149 - val_acc: 0.7566\n",
            "Epoch 56/75\n",
            "313/312 [==============================] - 26s 84ms/step - loss: 0.6067 - acc: 0.8160 - val_loss: 0.5647 - val_acc: 0.8134\n",
            "Epoch 57/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.6208 - acc: 0.8176 - val_loss: 0.4980 - val_acc: 0.8436\n",
            "Epoch 58/75\n",
            "313/312 [==============================] - 26s 84ms/step - loss: 0.5879 - acc: 0.8175 - val_loss: 0.4477 - val_acc: 0.8544\n",
            "Epoch 59/75\n",
            "313/312 [==============================] - 25s 79ms/step - loss: 0.5828 - acc: 0.8215 - val_loss: 0.6367 - val_acc: 0.7932\n",
            "Epoch 60/75\n",
            "313/312 [==============================] - 26s 84ms/step - loss: 0.5906 - acc: 0.8222 - val_loss: 0.5644 - val_acc: 0.8234\n",
            "Epoch 61/75\n",
            "313/312 [==============================] - 25s 81ms/step - loss: 0.5804 - acc: 0.8219 - val_loss: 0.8909 - val_acc: 0.7532\n",
            "Epoch 62/75\n",
            "313/312 [==============================] - 27s 88ms/step - loss: 0.5779 - acc: 0.8233 - val_loss: 0.5130 - val_acc: 0.8368\n",
            "Epoch 63/75\n",
            "313/312 [==============================] - 25s 79ms/step - loss: 0.5931 - acc: 0.8215 - val_loss: 0.5289 - val_acc: 0.8336\n",
            "Epoch 64/75\n",
            "313/312 [==============================] - 26s 84ms/step - loss: 0.6259 - acc: 0.8192 - val_loss: 0.6245 - val_acc: 0.8083\n",
            "Epoch 65/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.5825 - acc: 0.8195 - val_loss: 0.5568 - val_acc: 0.8189\n",
            "Epoch 66/75\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.5796 - acc: 0.8204 - val_loss: 0.7530 - val_acc: 0.7747\n",
            "Epoch 67/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.5922 - acc: 0.8227 - val_loss: 0.4883 - val_acc: 0.8380\n",
            "Epoch 68/75\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.5641 - acc: 0.8276 - val_loss: 0.4769 - val_acc: 0.8499\n",
            "Epoch 69/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.5624 - acc: 0.8258 - val_loss: 0.4838 - val_acc: 0.8413\n",
            "Epoch 70/75\n",
            "313/312 [==============================] - 26s 84ms/step - loss: 0.5654 - acc: 0.8252 - val_loss: 0.4820 - val_acc: 0.8446\n",
            "Epoch 71/75\n",
            "313/312 [==============================] - 25s 79ms/step - loss: 0.5624 - acc: 0.8266 - val_loss: 0.4595 - val_acc: 0.8552\n",
            "Epoch 72/75\n",
            "313/312 [==============================] - 26s 84ms/step - loss: 0.5690 - acc: 0.8280 - val_loss: 0.5302 - val_acc: 0.8347\n",
            "Epoch 73/75\n",
            "313/312 [==============================] - 26s 82ms/step - loss: 0.5525 - acc: 0.8296 - val_loss: 0.4873 - val_acc: 0.8538\n",
            "Epoch 74/75\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 0.5486 - acc: 0.8314 - val_loss: 0.4363 - val_acc: 0.8566\n",
            "Epoch 75/75\n",
            "313/312 [==============================] - 25s 80ms/step - loss: 0.5617 - acc: 0.8292 - val_loss: 0.6123 - val_acc: 0.8085\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "y4jTaUIzw_QR",
        "colab_type": "code",
        "outputId": "b4344d4c-31d2-4d20-ba13-278734b4157a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history.history['acc']\n",
        "val_acc = history.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "AW9Sl1TR_mzs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def lr_schedule(epoch):\n",
        "    lr = 1e-3\n",
        "    if epoch > 95:\n",
        "        lr *= 0.5e-3\n",
        "    elif epoch > 80:\n",
        "        lr *= 1e-3\n",
        "    elif epoch > 60:\n",
        "        lr *= 1e-2\n",
        "    elif epoch > 20:\n",
        "        lr *= 1e-1\n",
        "    print('Learning rate: ', lr)\n",
        "    return lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HYgPc15Ae0N5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b3230f6e-583b-44f2-b150-cc75660478e4"
      },
      "cell_type": "code",
      "source": [
        "reg2=None\n",
        "num_filters2=32\n",
        "ac2='relu'\n",
        "adm2=optimizers.Adam(lr=lr_schedule(0),decay=0, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
        "opt2=adm2\n",
        "drop_dense2=0.5\n",
        "drop_conv2=0\n",
        "\n",
        "model2 = Sequential()\n",
        "\n",
        "model2.add(Conv2D(num_filters2, (3, 3), activation=ac2, kernel_regularizer=reg2, input_shape=x_train.shape[1:],padding='same'))\n",
        "model2.add(BatchNormalization(axis=-1))\n",
        "model2.add(Conv2D(num_filters2, (3, 3), activation=ac2,kernel_regularizer=reg2,padding='same'))\n",
        "model2.add(BatchNormalization(axis=-1))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))   \n",
        "model2.add(Dropout(drop_conv2))\n",
        "\n",
        "model2.add(Conv2D(2*num_filters2, (3, 3), activation=ac2,kernel_regularizer=reg2,padding='same'))\n",
        "model2.add(BatchNormalization(axis=-1))\n",
        "model2.add(Conv2D(2*num_filters2, (3, 3), activation=ac2,kernel_regularizer=reg2,padding='same'))\n",
        "model2.add(BatchNormalization(axis=-1))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))   \n",
        "model2.add(Dropout(drop_conv2))\n",
        "\n",
        "model2.add(Conv2D(4*num_filters2, (3, 3), activation=ac2,kernel_regularizer=reg2,padding='same'))\n",
        "model2.add(BatchNormalization(axis=-1))\n",
        "model2.add(Conv2D(4*num_filters2, (3, 3), activation=ac2,kernel_regularizer=reg2,padding='same'))\n",
        "model2.add(BatchNormalization(axis=-1))\n",
        "model2.add(MaxPooling2D(pool_size=(2, 2)))  \n",
        "model2.add(Dropout(drop_conv2))\n",
        "\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(512, activation=ac2,kernel_regularizer=reg2))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Dropout(drop_dense2))\n",
        "model2.add(Dense(10, activation='softmax'))\n",
        "\n",
        "model2.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt2)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Learning rate:  0.001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nThbuYxWflVp",
        "colab_type": "code",
        "outputId": "99e56770-68fb-469c-ffc6-df82f52d178d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        }
      },
      "cell_type": "code",
      "source": [
        "model2.summary()\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_18 (Conv2D)           (None, 32, 32, 32)        896       \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "conv2d_19 (Conv2D)           (None, 32, 32, 32)        9248      \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 32, 32, 32)        128       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_8 (Dropout)          (None, 16, 16, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_20 (Conv2D)           (None, 16, 16, 64)        18496     \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_21 (Conv2D)           (None, 16, 16, 64)        36928     \n",
            "_________________________________________________________________\n",
            "batch_normalization_20 (Batc (None, 16, 16, 64)        256       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_9 (Dropout)          (None, 8, 8, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_22 (Conv2D)           (None, 8, 8, 128)         73856     \n",
            "_________________________________________________________________\n",
            "batch_normalization_21 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "conv2d_23 (Conv2D)           (None, 8, 8, 128)         147584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_22 (Batc (None, 8, 8, 128)         512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_11 (MaxPooling (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 4, 4, 128)         0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (None, 512)               1049088   \n",
            "_________________________________________________________________\n",
            "batch_normalization_23 (Batc (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 10)                5130      \n",
            "=================================================================\n",
            "Total params: 1,345,066\n",
            "Trainable params: 1,343,146\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U0C8JmoKfuvf",
        "colab_type": "code",
        "outputId": "a66e40f2-7224-4391-8970-faba910a45d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3572
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "history2=model2.fit_generator(datagen.flow(x_tr, y_tr, batch_size=128),\n",
        "                    steps_per_epoch = len(x_tr) / 128, epochs=100, validation_data=(x_val, y_val))"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "313/312 [==============================] - 36s 114ms/step - loss: 1.8330 - acc: 0.4032 - val_loss: 1.4391 - val_acc: 0.5233\n",
            "Epoch 2/100\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 1.3576 - acc: 0.5375 - val_loss: 1.1565 - val_acc: 0.6102\n",
            "Epoch 3/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 1.1628 - acc: 0.6048 - val_loss: 0.9983 - val_acc: 0.6666\n",
            "Epoch 4/100\n",
            "313/312 [==============================] - 28s 90ms/step - loss: 1.0344 - acc: 0.6441 - val_loss: 0.8077 - val_acc: 0.7213\n",
            "Epoch 5/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.9266 - acc: 0.6778 - val_loss: 0.7795 - val_acc: 0.7286\n",
            "Epoch 6/100\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 0.8396 - acc: 0.7079 - val_loss: 0.6900 - val_acc: 0.7633\n",
            "Epoch 7/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.7844 - acc: 0.7247 - val_loss: 0.6652 - val_acc: 0.7683\n",
            "Epoch 8/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.7386 - acc: 0.7429 - val_loss: 0.6214 - val_acc: 0.7809\n",
            "Epoch 9/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.7053 - acc: 0.7538 - val_loss: 0.5903 - val_acc: 0.8005\n",
            "Epoch 10/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.6787 - acc: 0.7658 - val_loss: 0.5741 - val_acc: 0.8009\n",
            "Epoch 11/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.6462 - acc: 0.7753 - val_loss: 0.5512 - val_acc: 0.8083\n",
            "Epoch 12/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.6227 - acc: 0.7841 - val_loss: 0.5309 - val_acc: 0.8144\n",
            "Epoch 13/100\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 0.6037 - acc: 0.7904 - val_loss: 0.5304 - val_acc: 0.8175\n",
            "Epoch 14/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.5882 - acc: 0.7980 - val_loss: 0.5205 - val_acc: 0.8207\n",
            "Epoch 15/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.5711 - acc: 0.8016 - val_loss: 0.4936 - val_acc: 0.8324\n",
            "Epoch 16/100\n",
            "313/312 [==============================] - 28s 90ms/step - loss: 0.5498 - acc: 0.8116 - val_loss: 0.4778 - val_acc: 0.8375\n",
            "Epoch 17/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.5352 - acc: 0.8138 - val_loss: 0.4859 - val_acc: 0.8352\n",
            "Epoch 18/100\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 0.5212 - acc: 0.8193 - val_loss: 0.4775 - val_acc: 0.8375\n",
            "Epoch 19/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.5086 - acc: 0.8243 - val_loss: 0.4673 - val_acc: 0.8382\n",
            "Epoch 20/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.4968 - acc: 0.8290 - val_loss: 0.4545 - val_acc: 0.8480\n",
            "Epoch 21/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.4852 - acc: 0.8331 - val_loss: 0.4478 - val_acc: 0.8497\n",
            "Epoch 22/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.4754 - acc: 0.8354 - val_loss: 0.4613 - val_acc: 0.8448\n",
            "Epoch 23/100\n",
            "313/312 [==============================] - 28s 89ms/step - loss: 0.4621 - acc: 0.8402 - val_loss: 0.4360 - val_acc: 0.8502\n",
            "Epoch 24/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.4599 - acc: 0.8407 - val_loss: 0.4261 - val_acc: 0.8577\n",
            "Epoch 25/100\n",
            "313/312 [==============================] - 28s 89ms/step - loss: 0.4479 - acc: 0.8444 - val_loss: 0.4298 - val_acc: 0.8546\n",
            "Epoch 26/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.4337 - acc: 0.8501 - val_loss: 0.4167 - val_acc: 0.8595\n",
            "Epoch 27/100\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 0.4315 - acc: 0.8489 - val_loss: 0.4154 - val_acc: 0.8611\n",
            "Epoch 28/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.4253 - acc: 0.8511 - val_loss: 0.4201 - val_acc: 0.8614\n",
            "Epoch 29/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.4104 - acc: 0.8586 - val_loss: 0.4089 - val_acc: 0.8640\n",
            "Epoch 30/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.4034 - acc: 0.8595 - val_loss: 0.4081 - val_acc: 0.8660\n",
            "Epoch 31/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.3977 - acc: 0.8622 - val_loss: 0.4111 - val_acc: 0.8633\n",
            "Epoch 32/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.4013 - acc: 0.8593 - val_loss: 0.4050 - val_acc: 0.8628\n",
            "Epoch 33/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.3866 - acc: 0.8651 - val_loss: 0.3863 - val_acc: 0.8707\n",
            "Epoch 34/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.3780 - acc: 0.8690 - val_loss: 0.4019 - val_acc: 0.8655\n",
            "Epoch 35/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.3789 - acc: 0.8683 - val_loss: 0.3879 - val_acc: 0.8706\n",
            "Epoch 36/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.3742 - acc: 0.8714 - val_loss: 0.4032 - val_acc: 0.8670\n",
            "Epoch 37/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.3690 - acc: 0.8724 - val_loss: 0.3946 - val_acc: 0.8678\n",
            "Epoch 38/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.3599 - acc: 0.8753 - val_loss: 0.3968 - val_acc: 0.8682\n",
            "Epoch 39/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.3635 - acc: 0.8735 - val_loss: 0.3988 - val_acc: 0.8667\n",
            "Epoch 40/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.3535 - acc: 0.8767 - val_loss: 0.3986 - val_acc: 0.8700\n",
            "Epoch 41/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.3532 - acc: 0.8759 - val_loss: 0.3874 - val_acc: 0.8739\n",
            "Epoch 42/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.3464 - acc: 0.8793 - val_loss: 0.3946 - val_acc: 0.8701\n",
            "Epoch 43/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.3404 - acc: 0.8830 - val_loss: 0.3831 - val_acc: 0.8722\n",
            "Epoch 44/100\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 0.3388 - acc: 0.8824 - val_loss: 0.3764 - val_acc: 0.8765\n",
            "Epoch 45/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.3273 - acc: 0.8851 - val_loss: 0.3741 - val_acc: 0.8762\n",
            "Epoch 46/100\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 0.3257 - acc: 0.8872 - val_loss: 0.3809 - val_acc: 0.8734\n",
            "Epoch 47/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.3261 - acc: 0.8870 - val_loss: 0.3725 - val_acc: 0.8757\n",
            "Epoch 48/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.3188 - acc: 0.8885 - val_loss: 0.3758 - val_acc: 0.8764\n",
            "Epoch 49/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.3178 - acc: 0.8893 - val_loss: 0.3745 - val_acc: 0.8768\n",
            "Epoch 50/100\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 0.3204 - acc: 0.8902 - val_loss: 0.3764 - val_acc: 0.8766\n",
            "Epoch 51/100\n",
            "313/312 [==============================] - 27s 88ms/step - loss: 0.3203 - acc: 0.8884 - val_loss: 0.3770 - val_acc: 0.8709\n",
            "Epoch 52/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.3087 - acc: 0.8911 - val_loss: 0.3762 - val_acc: 0.8773\n",
            "Epoch 53/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.3028 - acc: 0.8956 - val_loss: 0.3682 - val_acc: 0.8822\n",
            "Epoch 54/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.3028 - acc: 0.8952 - val_loss: 0.3736 - val_acc: 0.8788\n",
            "Epoch 55/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.3006 - acc: 0.8949 - val_loss: 0.3634 - val_acc: 0.8812\n",
            "Epoch 56/100\n",
            "313/312 [==============================] - 27s 88ms/step - loss: 0.3003 - acc: 0.8954 - val_loss: 0.3684 - val_acc: 0.8815\n",
            "Epoch 57/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2951 - acc: 0.8962 - val_loss: 0.3723 - val_acc: 0.8837\n",
            "Epoch 58/100\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 0.2901 - acc: 0.8979 - val_loss: 0.3730 - val_acc: 0.8807\n",
            "Epoch 59/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.2915 - acc: 0.8971 - val_loss: 0.3705 - val_acc: 0.8822\n",
            "Epoch 60/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2885 - acc: 0.8989 - val_loss: 0.3654 - val_acc: 0.8813\n",
            "Epoch 61/100\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 0.2866 - acc: 0.8997 - val_loss: 0.3662 - val_acc: 0.8818\n",
            "Epoch 62/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.2821 - acc: 0.9014 - val_loss: 0.3719 - val_acc: 0.8829\n",
            "Epoch 63/100\n",
            "313/312 [==============================] - 27s 88ms/step - loss: 0.2768 - acc: 0.9024 - val_loss: 0.3607 - val_acc: 0.8857\n",
            "Epoch 64/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2755 - acc: 0.9033 - val_loss: 0.3608 - val_acc: 0.8829\n",
            "Epoch 65/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2736 - acc: 0.9056 - val_loss: 0.3479 - val_acc: 0.8873\n",
            "Epoch 66/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.2727 - acc: 0.9036 - val_loss: 0.3682 - val_acc: 0.8832\n",
            "Epoch 67/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2793 - acc: 0.9012 - val_loss: 0.3735 - val_acc: 0.8793\n",
            "Epoch 68/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.2705 - acc: 0.9045 - val_loss: 0.3567 - val_acc: 0.8828\n",
            "Epoch 69/100\n",
            "313/312 [==============================] - 26s 84ms/step - loss: 0.2662 - acc: 0.9068 - val_loss: 0.3525 - val_acc: 0.8858\n",
            "Epoch 70/100\n",
            "313/312 [==============================] - 28s 89ms/step - loss: 0.2620 - acc: 0.9084 - val_loss: 0.3673 - val_acc: 0.8840\n",
            "Epoch 71/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2599 - acc: 0.9087 - val_loss: 0.3688 - val_acc: 0.8827\n",
            "Epoch 72/100\n",
            "313/312 [==============================] - 28s 88ms/step - loss: 0.2559 - acc: 0.9101 - val_loss: 0.3661 - val_acc: 0.8855\n",
            "Epoch 73/100\n",
            "313/312 [==============================] - 27s 88ms/step - loss: 0.2602 - acc: 0.9095 - val_loss: 0.3617 - val_acc: 0.8839\n",
            "Epoch 74/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2538 - acc: 0.9101 - val_loss: 0.3662 - val_acc: 0.8839\n",
            "Epoch 75/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.2569 - acc: 0.9113 - val_loss: 0.3611 - val_acc: 0.8840\n",
            "Epoch 76/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2504 - acc: 0.9132 - val_loss: 0.3592 - val_acc: 0.8855\n",
            "Epoch 77/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.2500 - acc: 0.9122 - val_loss: 0.3695 - val_acc: 0.8816\n",
            "Epoch 78/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2487 - acc: 0.9128 - val_loss: 0.3676 - val_acc: 0.8840\n",
            "Epoch 79/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.2492 - acc: 0.9128 - val_loss: 0.3640 - val_acc: 0.8853\n",
            "Epoch 80/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2435 - acc: 0.9123 - val_loss: 0.3616 - val_acc: 0.8900\n",
            "Epoch 81/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.2465 - acc: 0.9151 - val_loss: 0.3628 - val_acc: 0.8850\n",
            "Epoch 82/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.2391 - acc: 0.9157 - val_loss: 0.3562 - val_acc: 0.8845\n",
            "Epoch 83/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.2375 - acc: 0.9157 - val_loss: 0.3555 - val_acc: 0.8884\n",
            "Epoch 84/100\n",
            "313/312 [==============================] - 28s 89ms/step - loss: 0.2395 - acc: 0.9166 - val_loss: 0.3639 - val_acc: 0.8854\n",
            "Epoch 85/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.2345 - acc: 0.9165 - val_loss: 0.3618 - val_acc: 0.8856\n",
            "Epoch 86/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.2385 - acc: 0.9153 - val_loss: 0.3659 - val_acc: 0.8845\n",
            "Epoch 87/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.2282 - acc: 0.9207 - val_loss: 0.3596 - val_acc: 0.8889\n",
            "Epoch 88/100\n",
            "313/312 [==============================] - 27s 86ms/step - loss: 0.2287 - acc: 0.9204 - val_loss: 0.3717 - val_acc: 0.8830\n",
            "Epoch 89/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.2357 - acc: 0.9182 - val_loss: 0.3569 - val_acc: 0.8862\n",
            "Epoch 90/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2291 - acc: 0.9187 - val_loss: 0.3640 - val_acc: 0.8856\n",
            "Epoch 91/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.2256 - acc: 0.9209 - val_loss: 0.3683 - val_acc: 0.8867\n",
            "Epoch 92/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2244 - acc: 0.9218 - val_loss: 0.3654 - val_acc: 0.8868\n",
            "Epoch 93/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.2265 - acc: 0.9199 - val_loss: 0.3593 - val_acc: 0.8899\n",
            "Epoch 94/100\n",
            "313/312 [==============================] - 27s 87ms/step - loss: 0.2214 - acc: 0.9221 - val_loss: 0.3479 - val_acc: 0.8894\n",
            "Epoch 95/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2190 - acc: 0.9230 - val_loss: 0.3632 - val_acc: 0.8895\n",
            "Epoch 96/100\n",
            "313/312 [==============================] - 28s 90ms/step - loss: 0.2206 - acc: 0.9217 - val_loss: 0.3560 - val_acc: 0.8884\n",
            "Epoch 97/100\n",
            "313/312 [==============================] - 26s 84ms/step - loss: 0.2196 - acc: 0.9231 - val_loss: 0.3506 - val_acc: 0.8890\n",
            "Epoch 98/100\n",
            "313/312 [==============================] - 27s 88ms/step - loss: 0.2206 - acc: 0.9225 - val_loss: 0.3641 - val_acc: 0.8858\n",
            "Epoch 99/100\n",
            "313/312 [==============================] - 27s 85ms/step - loss: 0.2203 - acc: 0.9224 - val_loss: 0.3589 - val_acc: 0.8898\n",
            "Epoch 100/100\n",
            "313/312 [==============================] - 26s 85ms/step - loss: 0.2163 - acc: 0.9245 - val_loss: 0.3533 - val_acc: 0.8928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3Rizc_XYDbeN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a3644958-eaf3-4ef5-a651-8183a8ffc84a"
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "acc = history2.history['acc']\n",
        "val_acc = history2.history['val_acc']\n",
        "\n",
        "epochs = range(len(acc))\n",
        "\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'r', label='Validation acc')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEGCAYAAABsLkJ6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nD1T0Q0f/lFIsAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "aKVrvzxdl-YZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Train (again) and evaluate the model\n",
        "\n",
        "- To this end, you have found the \"best\" hyper-parameters. \n",
        "- Now, fix the hyper-parameters and train the network on the entire training set (all the 50K training samples)\n",
        "- Evaluate your model on the test set."
      ]
    },
    {
      "metadata": {
        "id": "9NEnXoZgl-Yf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.1. Train the model on the entire training set\n",
        "\n",
        "Why? Previously, you used 40K samples for training; you wasted 10K samples for the sake of hyper-parameter tuning. Now you already know the hyper-parameters, so why not using all the 50K samples for training?"
      ]
    },
    {
      "metadata": {
        "id": "rS0KMndBC_rN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model2.compile(loss='categorical_crossentropy', metrics=['accuracy'],optimizer=opt2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iBgRZG-0l-Y0",
        "colab_type": "code",
        "outputId": "d6814186-7a99-4167-fc4f-4bd420cd2ef0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        }
      },
      "cell_type": "code",
      "source": [
        "history3 = model2.fit(x_train, y_train_vec, batch_size=128, epochs=5)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "50000/50000 [==============================] - 28s 563us/step - loss: 0.1264 - acc: 0.9559\n",
            "Epoch 2/5\n",
            "50000/50000 [==============================] - 23s 466us/step - loss: 0.0340 - acc: 0.9888\n",
            "Epoch 3/5\n",
            "50000/50000 [==============================] - 23s 465us/step - loss: 0.0175 - acc: 0.9948\n",
            "Epoch 4/5\n",
            "50000/50000 [==============================] - 23s 464us/step - loss: 0.0123 - acc: 0.9966\n",
            "Epoch 5/5\n",
            "50000/50000 [==============================] - 23s 460us/step - loss: 0.0109 - acc: 0.9965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NKkHD9yel-ZA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2. Evaluate the model on the test set\n",
        "\n",
        "Do NOT used the test set until now. Make sure that your model parameters and hyper-parameters are independent of the test set."
      ]
    },
    {
      "metadata": {
        "id": "bFwceEgzl-ZE",
        "colab_type": "code",
        "outputId": "fe815b97-30d3-46db-bfdc-f7268e2c1aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "cell_type": "code",
      "source": [
        "loss_and_acc = model2.evaluate(x_test, y_test_vec)\n",
        "print('loss = ' + str(loss_and_acc[0]))\n",
        "print('accuracy = ' + str(loss_and_acc[1]))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 4s 415us/step\n",
            "loss = 0.6905957343637943\n",
            "accuracy = 0.8737\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}